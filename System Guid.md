这是关于Lezer解析器系统的指南。它提供了该系统功能的简要描述。有关其接口的逐项文档，请参阅参考手册。

# Overview

Lezer是一个用JavaScript编写的解析器系统。在给定语法的形式描述的情况下，它可以生成一组解析表。这些表提供了解析器系统可以使用的描述信息，以便有效地构建给定文本的语法树，在语法层面描述文本的结构。

@lezer/generator工具用于生成这些表，它是一个构建工具，接受一个文件，该文件的格式稍后在本指南中描述，并输出一个表示解析表的大型、难以阅读的JavaScript代码文件。这是一个离线过程，作为语法包构建过程的一部分。

@lezer/lr包提供了运行时解析系统。结合由生成器构建的解析器，它提供了一个解析器对象，可以接受一个源文件并返回一个语法树。

这些树由@lezer/common包中的数据结构表示，与您在其他上下文中可能见过的抽象语法树相比，它们更为有限。它们不是非常抽象。每个节点只存储一个类型、起始和结束位置以及一个扁平的子节点序列。在编写语法时，您可以选择将哪些产生式存储为节点，其他产生式根本不会出现在树中。

这意味着树在内存中非常紧凑且构建成本低廉。但是，对其进行精细分析有些困难。该库是为了编辑器系统而设计，它保留了已编辑文档的语法树，并将其用于语法高亮和智能缩进等功能。

为了实现这一目的，解析器具有一些其他有趣的特性。它可以进行增量解析，这意味着它可以高效地重新解析与某个先前版本相比稍有变化的文档，给定旧版本的解析结果。它还具有内置的错误恢复功能，这意味着即使输入不符合语法，它仍然可以为其生成一些合理的语法树。

该系统的方案受到了[tree-sitter](http://tree-sitter.github.io/tree-sitter/)的很大影响，tree-sitter是一个类似的用C和Rust编写的系统，并且受到Tim Wagner和Susan Graham关于增量解析的几篇论文的影响[1](https://lezer.codemirror.net/docs/guide/ftp.cs.berkeley.edu/sggs/toplas-parsing.ps) [2](https://www.semanticscholar.org/paper/Incremental-Analysis-of-real-Programming-Languages-Wagner-Graham/163592ac3777ee396f32318fcd83b1c563f2e496)。它作为一个不同的系统存在，因为它具有与tree-sitter不同的侧重点 -- 作为JavaScript系统的一部分，它是用JavaScript编写的，具有相对较小的库和解析表大小。它还生成更紧凑的内存中树，以避免给用户的机器造成过大的压力。

# Parser Algorithm

Lezer基于[LR](https://en.wikipedia.org/wiki/LR_parser)分析，这是Donald Knuth在1965年发明的一种算法。通过预先分析语法，可以为某些语法推导出完全确定性（因此高效）的解析器。

大致而言，该算法对语法进行[抽象解释](https://en.wikipedia.org/wiki/Abstract_interpretation)，记录解析器可能处于的各种状态，并为每个状态创建一个表，(这个表)将终结符号（标记）映射到解析器在该状态下遇到该标记时应采取的操作。如果在给定状态下对于给定标记有多个操作可执行，则该语法无法使用此算法进行解析。这类问题通常称为“shift-reduce”或“reduce-shift”冲突。稍后将详细介绍这些问题。

在为基于LR的工具编写语法时，对这个算法有一定的了解可能会有所帮助。这篇[维基百科文章](https://en.wikipedia.org/wiki/Abstract_interpretation)是一个很好的入门介绍。如果需要更深入的学习，我推荐阅读[这本书](https://dickgrune.com/Books/PTAPG_1st_Edition/BookBody.pdf)的第9章（PDF格式）。

## Ambiguity

许多语法要么无法表示为普通的LR语法，要么表示起来非常困难。如果一个元素的语法角色只有在解析的后期才变得清晰（例如JavaScript的`(x = 1) + 1`和`(x = 1) => x`，其中`(x = 1)`可以是表达式或参数列表），普通的LR通常不太实用。

GLR是解析算法的扩展，允许解析在模糊点“分裂”，即对于给定的标记应用多个动作。然后，替代解析继续并行进行。当解析无法再取得进展时，它被丢弃。只要至少有一个解析继续，我们就可以得到语法树。

GLR在处理局部歧义时非常有帮助，但是如果应用不当，它很容易导致并发解析的爆炸，并且在语法实际上是有歧义的情况下，多个解析会无限地继续，从而实际上同时对文档进行多次解析。这完全破坏了使LR解析有用的特性：可预测性和高效性。

Lezer允许GLR解析，但要求您在语法中明确注释允许发生GLR的位置，以便您可以使用它来解决其他情况下困难的问题，但它不会意外地在整个语法中发生。

Lezer对解析状态的分裂进行了优化，因此，尽管它比线性解析更昂贵，您仍然可以在语法结构中存在歧义的情况下获得快速的解析器。

## Error Recovery

在默认的非[严格模式](https://lezer.codemirror.net/docs/ref/#lr.ParserConfig.strict)下，解析器不会因为遇到错误而终止执行，相反，其会尽可能地解析完整的文档并生成语法树。

为了实现这一点，解析器在使用GLR机制的同时尝试各种恢复技巧（忽略当前标记或跳到与当前标记匹配的位置），以便在几个标记之后确定以哪种恢复方式能够得到最佳结果。

错误节点（包括解析器开始跳过的位置）也会被保存在语法树中。

## Incremental Parsing

为了避免重复解析，解析器允许您提供一个树片段的缓存，其中包含先前解析生成的树的信息，并带有有关文档中发生的更改的注释信息。解析器会尽可能地重用该缓存中的节点，而不是重新解析那部分文档。

由于语法树将重复运算符（在语法表示中用+和*指定）的匹配序列表示为平衡子树，重新匹配未更改的文档部分的成本很低，即使是对于大型文档，您也可以快速创建新的语法树。

然而，这并非绝对可靠 -- 即使是微小的文档更改，如果它改变了其后内容的含义，可能需要重新解析文档的大部分。例如，添加或删除一个块注释的起始标记。

## Contextual Tokenization

在传统的解析技术中，标记器（tokenizer）和解析器之间存在严格的分离。标记器负责将输入拆分为一系列原子标记，而解析器则负责解析这些标记。

然而，这种分离有时会带来问题。有时，文本的含义取决于上下文，比如JavaScript中正则表达式符号和除法运算符语法的歧义。而在其他情况下，语法中使用的子语言（比如字符串内容）对标记的概念可能与语法的其他部分不同。

Lezer支持上下文标记读取。只要这些标记在语法的任何地方不会出现在同一位置（也就是说不会对同一个字符串应用两种语法），就可以为同一标记应用不同的语法（从而产生含义不同的节点）。

您还可以定义外部标记器，这会导致解析器调用您的代码来读取标记。这样的标记器只会在它们产生的标记适用于当前位置时才会被调用。

甚至空白符，解析器隐式跳过的标记类型，也是上下文相关的，您可以定义不同的规则来跳过不同的内容。

# Writing a Grammer

Lezer的[解析器生成器](https://lezer.codemirror.net/docs/ref/#generator)为语法定义了自己的符号表示法。您可以查看[JavaScript语法](https://github.com/lezer-parser/javascript/blob/master/src/javascript.grammar)的示例。

语法是一组规则，用于定义术语。术语可以是标记（tokens），直接匹配输入文本的一部分，也可以是非终结符（nonterminals），用于匹配由其他术语组成的表达式。标记和非终结符使用类似的语法进行定义，但它们有明确的区别（标记必须出现在@tokens块中），而且标记在匹配能力上有一定的限制——例如，它们不能包含任意递归。

从形式上讲，标记必须匹配正则语言（基本正则表达式可以匹配的内容），而非终结符可以表示上下文无关语言。

以下是一个简单语法的示例：

```
@top Program { expression }

expression { Name | Number | BinaryExpression }

BinaryExpression { "(" expression ("+" | "-") expression ")" }

@tokens {
  Name { @asciiLetter+ }
  Number { @digit+ }
}
```

这将匹配类似于`(a+1)`或`(100-(2+4))`的字符串。它不允许在标记之间有空格，并且需要在二元表达式周围使用括号，否则它会报告歧义错误。

@top定义了语法的入口点。这是用于匹配整个输入的规则。通常它会包含一些重复的内容，比如`statement+`。

您会注意到示例中有一些以小写字母开头的术语，还有一些以大写字母开头的术语。这种区别是有意义的。大写字母开头的规则将显示为解析器生成的语法树中的节点，而小写字母开头的规则则不会。

## Operators

语法表示法支持重复运算符 *（前面的内容可以重复任意次数）、+（至少重复一次）、?（可选，重复零次或一次）。这些运算符具有高优先级，仅适用于直接位于它们之前的元素。

竖线 | 字符用于表示选择，匹配其两侧的任一表达式。当然，可以重复使用竖线以表示多于两个选择（x | y | z）。选择运算符具有最低的优先级，如果没有括号存在，则应用于其左右两侧的整个表达式。

上下文无关文法中的选择是可交换的，这意味着 a | b 与 b | a 完全等价。

括号可用于分组，例如 x (y | z)+。

通过将事物简单地放在一起，可以表示事物的序列。a b 表示 a 后面跟着 b。

## Tokens

命名标记是在@tokens块中定义的。您还可以在tokens块之外使用字符串字面量（如"+"）作为标记。这将自动定义一个新的标记。字符串字面量使用与JavaScript字符串相同的转义规则。

标记规则中的字符串字面量的工作方式有所不同。它们可以与其他表达式组合以形成更大的标记。所有标记规则（和字面标记）都编译为确定性有限自动机，然后可以使用该自动机高效地将其与文本流进行匹配。

因此，在非终结符规则中的表达式 "a" "b" "c" 是三个标记的序列。在标记规则中，它与字符串 "abc" 完全等价。

您可以使用**集合表示法**来表示字符集，这与正则表达式中的方括号表示法有些类似。`$[.,]` 表示句点或逗号（在此表示法中，句点没有特殊含义，转义字符如 \s 也没有特殊含义）。`$[a-z]` 匹配 a、z 和在 Unicode 字符编码顺序中位于它们之间的任何字符。要创建一个反转的字符集，只匹配未在集合中提到的字符，您需要在方括号之前使用感叹号而不是美元符号。因此，`![x]` 匹配任何不是 x 的字符。

解析器生成器定义了一些内置字符集，可以通过 @ 符号访问：

- @asciiLetter 匹配 $[a-zA-Z]
- @asciiLowercase 匹配 $[a-z]
- @asciiUppercase 等价于 $[A-Z]
- @digit 匹配 $[0-9]
- @whitespace 匹配 Unicode 标准定义的**任何空白字符**。
- @eof 匹配输入的结尾

标记规则不能引用非终结符规则。但是非终结符之间可以相互引用，只要这些引用不形成**非尾递归循环**。也就是说，规则 x 不能直接或间接地包含对 x 的引用，除非该引用出现在规则的最末尾。

## Skip Expressions

我们的初始示例不允许标记之间存在任何空格。几乎所有实际语言都定义了一些特殊的标记，通常涵盖空格和注释，这些标记可以出现在其他标记之间，并在匹配语法时被忽略。

要支持空格，您需要在语法中添加一个@skip规则。

```
@skip { space | Comment }
```

您可以像这样定义space和Comment标记：

```
@tokens {
  space { @whitespace+ }
  Comment { "//" ![\n]* }
  // ...
}
```

@skip规则将在其他标记之间匹配零次或多次。因此，上面的规则还可以处理一长串的注释和空格。

被跳过的标记可以大写（如Comment），这样它们将出现在语法树中。跳过表达式可以匹配比单个标记更复杂的内容。

当您的语法需要多种类型的空格时，例如当字符串不是简单的标记而是需要自己的内部结构时，但您不希望在字符串片段之间匹配空格和注释，您可以创建一个类似下面的跳过块：

@skip {} {
  String {
    stringOpen (stringEscape | stringContent)* stringClose
  }
}

初始大括号中包含跳过表达式，这种情况下我们不想跳过任何内容，因此为空，第二对大括号中包含在其中使用此跳过表达式的规则。

解析状态只能与一组跳过表达式关联（否则在该状态中应该跳过什么将变得不清楚）。这意味着具有自定义跳过表达式的规则（如上例）如果在另一个跳过上下文中使用，必须在两侧明确定界。例如，它不能在末尾具有可选或重复的项，因为在该点上可能会或可能不会遵循该项，不清楚应该跳过什么内容。

## Precedence

让我们回到示例中的二元操作符规则。如果我们像这样定义它，去掉括号，就会得到一个错误消息。

```
BinaryExpression { expression ("+" | "-") expression }
```

错误提示为"shift/reduce冲突"，在表达式 "+" 表达式 · "+" 处。·表示解析位置，解析器告诉我们，在读取了一个看起来像二元表达式的内容后，又遇到另一个操作符，它不知道是先将初始标记规约为`BinaryExpression`，还是先移动第二个操作符，留下第一个操作符以后再处理。基本上，它不知道是否将 `x + y + z` 解析为 `(x + y) + z` 还是 `x + (y + z)`。

幸运的是，这种问题可以在不使用GLR解析的情况下解决。解决这个问题涉及对LR解析器生成算法的一种基本粗糙的修改：通过指定优先级来解决这些冲突，这样当解析器生成器遇到歧义时，我们告诉它选择一个解析而非另一个。

这种方法不仅可以用于关联性（上述错误），还可以用于运算符优先级（例如，给予包含*的分组比包含+的分组更高的优先级）以及其他各种问题。

在Lezer中指定优先级的方法是首先使用@precedence块，按优先级的顺序（最高优先级在前）枚举您在语法中要使用的优先级名称，然后在模棱两可的位置插入优先级标记。

```
@precedence { times @left, plus @left }

@top Program { expression }

expression { Number | BinaryExpression }

BinaryExpression {
  expression !times "*" expression |
  expression !plus "+" expression
}

@tokens {
  Number { @digit+ }
}
```

这告诉解析器生成器，!times优先级标记比!plus高，并且两者都是左结合的（更喜欢(a + b) + c而不是a + (b + c)）。您还可以在优先级名称后面指定@right，使其成为右结合的，或者省略关联性以不指定任何关联性（在这种情况下，优先级将不会解决与自身的shift/reduce冲突）。

!标记必须插入到冲突发生的位置。在这种情况下，冲突发生在操作符的前面，因此标记就放在那里。

现在，这个语法可以正确解析类似`1+2*3+4`的内容，将操作符按照优先级进行分组，例如`(1+(2*3))+4`。

> 左结合（left associative）和右结合（right associative）是用于描述操作符在表达式中如何结合的概念。左结合意味着当有多个相同优先级的操作符出现在一个表达式中时，它们将按照从左到右的顺序进行结合。也就是说，左结合的操作符会优先与其左侧的操作数结合。例如，加法和减法通常是左结合的。对于表达式 "a + b + c"，左结合会将其解释为 "(a + b) + c"，先计算左边的加法，然后再与右边的操作数相加。右结合则相反，它意味着操作符会从右到左进行结合。右结合的操作符会优先与其右侧的操作数结合。例如，指数运算符通常是右结合的。对于表达式 "a ** b ** c"，右结合会将其解释为 "a ** (b ** c)"，先计算右边的指数运算，然后再进行左边的指数运算。左结合和右结合的区别在于操作符的结合顺序。这在解析表达式时非常重要，因为它决定了操作符的优先级和操作数的组合方式。正确的结合方式可以确保表达式按照预期的顺序进行计算。在语法规则中指定操作符的关联性可以帮助解析器在解析表达式时正确地应用结合规则，并避免冲突和歧义。

另外，您还可以使用关键字@cut，将某个优先级的解释设为剪切操作符，而不是指定关联性。剪切操作符将覆盖其他解释，即使尚未检测到冲突。一个适合使用剪切操作符的例子是JavaScript中的函数关键字，它在语句位置有不同的含义。语句可以以表达式开头，而表达式可以是函数表达式，但当我们在语句的开头看到function标记时，我们只想进入函数声明语句规则，而不是函数表达式规则。

```
@precedence { decl @cut }

@top Program { statement+ }

statement { FunctionDeclaration | FunctionExpression }

FunctionExpression { "function" "..." }

FunctionDeclaration { !decl "function" "..." }
```

这将将`function...`解析为`FunctionDeclaration`，尽管它也可以匹配`FunctionExpression`（如果它是真实的语法，可能会在其他地方使用）。

## Allowing Ambiguity

然而，有些问题无法通过优先级声明来解决。为了明确允许Lezer在给定位置尝试多个动作，您可以使用歧义标记。
```
@top Program { (GoodStatement | BadStatement)+ }

GoodStatement { "(" GoodValue ")" ";" }

GoodValue { "val" ~statement }

BadStatement { "(" BadValue ")" "!" }

BadValue { "val" ~statement }
```
这个语法完全没有意义，但它展示了问题所在：为了知道是匹配GoodValue还是BadValue，解析器必须决定它是解析GoodStatement还是BadStatement。但在它需要做出决定的时候，下一个标记是一个右括号，它还不能得知它所需的信息。

解析器生成器会对此发出警告（这是一个"reduce/reduce"冲突），除非我们在规约发生的地方添加~statement歧义标记。单词"statement"没有特殊的含义，除了它必须在所有冲突的位置上相同，这样解析器生成器就知道我们在显式地注释这两个位置为歧义。

有了这些标记，语法就可以编译了，每当解析器到达歧义位置时，它会分割解析状态，尝试两种方法，然后在看到无法匹配下一个标记时放弃其中一条路径。

在语法真正存在歧义的情况下，也就是多个解析可以继续到达相同状态或输入的末尾，很难预测Lezer将选择哪个解析作为最终解析。为了使某个变体占据优势，可以为规则添加动态优先级。这是使用prop表示法完成的（稍后我们会回到这个）。
```
@top Program { (A | B)+ }

A[@dynamicPrecedence=1] { "!" ~ambig }

B { "!" ~ambig }
```
这将解析感叹号为A，尽管它也匹配B，因为该规则具有较高的动态优先级。这样的优先级可以从-10到10的范围内取值。负数惩罚包含此规则的分支，正数给予其奖励。

## Template Rules

规则模板就是按照参数生成相似语法规则的规则。Lezer在参数和参数值周围使用尖括号。您可以像这样定义一个参数化规则：

```
commaSep<content> { "" | content ("," content)* }
```
（空字符串被视为空序列，或ε，不匹配任何内容。）

在定义了上述规则之后，您可以使用`commaSep<expression>`来匹配逗号分隔的表达式列表。

当您将参数传递给在@tokens块中定义的规则时，即使在非终结符规则中调用它，这些参数也将被解释为属于Token规则的一部分。

## Token Precedence

默认情况下，只有当令牌在语法的相同位置不出现，或者它们都是简单令牌（没有使用重复操作符定义）时，才允许令牌重叠（匹配彼此的一些前缀）。也就是说，你可以有令牌"+"和"+="，但不能有令牌"+"和"+" "="+。

为了明确指定在这种情况下，其中一个令牌优先，你可以在`@tokens`块中添加`@precedence`声明。

```lezer
@tokens {
  Divide { "/" }
  Comment { "//" ![\n]* }
  @precedence { Comment, Divide }
}
```

默认情况下，由于`Divide`是`Comment`的前缀，它们被认为是重叠的。`@precedence`声明指定当两者都匹配时，`Comment`优先于`Divide`。你可以在`@tokens`块中有多个`@precedence`声明，并且每个声明可以包含任意数量的令牌，以解决这些令牌之间的优先关系。

## Token Specialization

您可以尝试通过指定关键字优先级来处理与标识符重叠的关键字。但是这样做很麻烦，因为如果您将关键字定义为匹配"new"并且优先级高于标识符，那么输入"newest"将首先匹配关键字"new"，然后匹配标识符"est"。

此外，如果在 tokenizer 自动机中有几十个关键字，它会大大增加其大小和复杂性。许多手写的 tokenizer 的做法是先匹配标识符，然后检查其内容是否与任何关键字相对应。

在Lezer中，可以使用`@specialize`操作符来支持这一点。该操作符基于基本token和字符串字面量声明一个新的token。当为给定的token类型定义了specialization时，每次读取该令牌时都会查找其记录表，如果其内容与表中的spec记录匹配，则会被其对应的token替换。

```plaintext
NewExpression { @specialize<identifier, "new"> expression }
```

这个规则将匹配类似于"new Array"的内容。

还有另一个类似于`@specialize`的操作符，称为`@extend`。与spec token替换原始 token 不同，扩展token允许两种含义同时生效，当两者都适用时，隐式启用GLR。这对于上下文关键字很有用，直到几个token后才能确定它们是应该被视为标识符还是关键字。

## External Tokens

通常情况下，常规的令牌定义方式足够使用且最为便捷。然而，常规语言在匹配能力上有明显的限制，因此有些情况下，比如需要多字符向前查找才能识别注释结束符的情况，在语法中表达起来会显得笨拙。而且，类似于JavaScript的自动分号插入机制，需要检查前导空白内容并查看后面的字符，根本无法在语法中表达。

因此，Lezer允许您将某些令牌声明为外部令牌，并使用您提供的JavaScript代码进行读取。外部令牌化器的作用就是解决这些问题。它可以处理更复杂的令牌化场景，例如多字符向前查找或上下文敏感的行为。

通过将常规的令牌化方法与外部令牌化器相结合，Lezer提供了一种更灵活、更强大的机制，用于处理常规语言无法轻易表达的复杂语言特性和行为。

Lezer允许您将某些令牌声明为外部令牌，由您提供的JavaScript代码来读取。外部令牌化器的声明可能如下所示：

```plaintext
@external tokens insertSemicolon from "./tokens" { insertedSemicolon }
```

这告诉解析器生成器应该从"./tokens"导入`insertSemicolon`符号，并将其用作 tokenizer，该tokenizer可能生成`insertedSemicolon`令牌。

这样的令牌化器应该是`ExternalTokenizer`实例。仅当当前状态允许匹配它定义的令牌之一时，才会调用它。

`@external tokens`声明和`@tokens`块在语法文件中的出现顺序很重要。先出现的声明将优先于后面出现的声明，这意味着如果先出现的声明返回一个令牌，其他声明将不会被尝试。

还可以定义外部专门化逻辑。使用类似于下面的指令，每当读取一个标识符令牌时，将调用给定的函数（`specializeIdent`），它可以返回替换令牌或-1以指示它不专门化该值。

```plaintext
@external specialize {identifier} specializeIdent from "./tokens" {
  keyword1,
  keyword2
}
```

在括号中列出的令牌提供了专门化器可能返回的令牌。您还可以使用`extend`而不是`specialize`，使其成为扩展专门化器，其中原始令牌和专门化令牌都会尝试。

## Context

有时，在创建Python解析器中的“缩进”和“取消缩进”令牌时，解析过程需要保持一些状态（例如当前缩进），并在外部令牌化器中访问该状态。

在Lezer中，这通过上下文（context）来实现。上下文是与解析过程一起保持并根据解析器执行的操作进行更新的值。它以外部代码的形式编写，作为上下文追踪器（context tracker），它是描述如何创建和更新值的对象。在语法中使用 @context 声明启用上下文追踪。

例如，使用以下语法声明来跟踪缩进：
```
@context trackIndent from "./helpers.js"
```
如果上下文对于增量解析中的节点重用的有效性有影响（例如，如果周围的缩进不同，就不能重用Python块），则上下文应导出一个哈希函数，用于为上下文值创建一个数值哈希。这些哈希值存储在树节点中，节点的哈希值必须与当前上下文匹配才能进行节点重用。

## Local Token Groups

有时，您希望定义一个token类型来覆盖其他token无法匹配的所有文本。一个例子是字符串的内容，其中与转义符、插值或结束引号不匹配的任何内容都应该作为内容token处理。在简单的情况下，您可以为此编写一个普通的Lezer token，但是严格的正则语言对于表达“匹配到这些token之前的任何内容”的情况非常不适合，有时甚至无法表达。

本地token组允许您定义一组在给定上下文中使用的token（例如字符串或注释的内容），以及一个用于处理其他情况的回退token。

以下是一个使用本地令牌化器的示例：
```
@local tokens {
  stringEnd[@name='"'] { '"' }
  StringEscape { "\\" _ }
  @else stringContent
}

@skip {} {
  String { '"' (stringContent | stringEscape)* stringEnd }
}
```
这样的本地tokenizer仅在解析状态中没有其他token（无跳过token、无文字token和无在其他tokenizer块中定义的token）时才起作用。因此，您几乎总是需要在使用本地令牌化器的规则周围添加 @skip {} 块，并且可能需要重新定义已经存在的令牌（例如示例中的 '"'）在本地令牌块中。

您可以在本地token定义中引用在其他token块中定义的规则，以避免在其他上下文中使用的复杂token的符号重复。

## Inline Rules

对于只会使用一次的规则，可以完全在使用它们的规则内部编写。这在为子表达式声明节点名称时非常有用。例如，下面是一个语句规则的定义（它本身不会创建树节点），它为每个选择项指定了名称：
```
statement {
  ReturnStatement { Return expression } |
  IfStatement { If expression statement } |
  ExpressionStatement { expression ";" }
}
```
在上述示例中，statement规则包含了三个选择项：ReturnStatement、IfStatement和ExpressionStatement。每个选择项都在大括号内定义了其规则。这样做的好处是，您可以为每个选择项指定一个节点名称，以便在解析过程中创建相应的树节点。

## Node Props

可以在语法文件中直接添加节点属性（node props），这些属性与树节点相关联并提供额外的元数据。为此，您可以在规则名称后面加上方括号。
```
StartTag[closedBy="EndTag"] { "<" }
```
这将为该节点类型添加`NodeProp.closedBy`属性，该属性提供了关于包围节点的分隔符的信息。只包含字母的值可以不用引号写，像上面的示例中的值必须用引号引起来。

大多数作为`NodeProp`静态属性定义的属性默认是可用的。您还可以使用以下语法导入自定义属性：
```
@external prop myProp from "./props"

SomeRule[myProp=somevalue] { "ok" }
```
您可以在属性名称后面写上`otherName`来在本地重命名它。

内联规则、外部token名称以及@specialize/@extend操作符也允许使用方括号属性表示法来为定义的节点添加属性。

伪属性（以@开头的名称），当它们出现在方括号之间时，用于激活和配置各种类型的附加功能。

例如，@name可用于设置节点的名称。下面的规则将生成一个名为x的节点，尽管它本身被称为y。当明确指定名称时，无论节点是否大写，它都将包含在树中。
```
y[@name=x] { "y" }
```
有时候将规则参数的值插入属性中是很有用的。您可以在属性值中使用花括号来插入标识符或文字参数表达式的内容。以下关键字辅助规则将生成一个以关键字命名的特殊token：
```
kw<word> { @specialize[@name={word}]<identifier, word> }
```
如果您在语法文件的顶层放置了`@detectDelim`指令，解析器生成器将自动检测规则的分隔符token，并为找到的规则创建openedBy和closedBy属性。

## Literal Token Declarations

字面令牌默认情况下不会创建节点。但是，您可以在tokens块中显式列出您希望包含在树中的字面令牌，可选地附带属性。
```
@tokens {
  "("
  ")"
  "#"[someProp=ok]

  // ... 其他令牌规则
}
```
## Dialects

有时候在单个语法中定义多个类似的语言是有意义的，这样它们可以共享代码并作为单个序列化解析器加载。在这种情况下，您需要一种方式来使语法的部分条件化，以便根据正在解析的语言选择打开或关闭这些部分。

Lezer具有方言（dialect）功能，旨在帮助处理这个问题。它允许您使某些令牌依赖于所选的方言。使用方式如下：
```
@dialects { comments }

@top Document { Word+ }

@skip { space | Comment }

@tokens {
  Comment[@dialect=comments] { "//" ![\n]* }
  Word { @asciiLetter+ }
}
```
@dialects声明提供了语法支持的方言列表。可以使用@dialect伪属性为单个token（以及通过特化其他token生成的token）添加注释，以指示它们只能在该方言激活时出现。多个方言可以同时激活。

外部token解析器和特化器可以通过`Stack.dialectEnabled`方法访问活动的方言，使用从`terms`文件导出的方言ID（例如`stack.dialectEnabled(Dialect_comments)`），并基于此决定是否返回给定的令牌。这在您需要对方言执行更复杂的测试时也很有用（例如仅在方言未激活时返回token，或者当多个方言同时激活时返回token）。

## Grammar Nesting

在某些情况下，比如HTML中嵌入的JavaScript或代码片段，您希望由另一个解析器负责解析文档的一部分。

Lezer通过对树进行后处理来实现这一点。parseMixed方法会扫描树（或在增量解析的场景下解析新增的树）以查找应该嵌入其他语言的节点。

如果找到使用其他语言的范围，则会在其上运行相应的解析器，并使用`NodeProp.mounted`将生成的树附加到主树上。有两种挂载子树的方式：

- 常规挂载（regular mounts）在遍历外部树时，只需用内部树的根节点替换原始节点。这是最容易使用的方式，通常在语言严格嵌套时更可取。例如，在HTML树中，<script>标签的内容可以用JavaScript树替换。

- 覆盖（overlays）仅替换节点的部分内容为另一个树的内容。这在嵌套结构不是严格层次化的情况下非常有用，比如模板语言和其输出语言之间，两者都有结构，但结构可能以奇怪的方式重叠（例如：<p>...<?if x?></p><p>...<?/if?></p>）。

在正常遍历树时，会忽略覆盖。但您可以使用enter方法显式进入覆盖。

























